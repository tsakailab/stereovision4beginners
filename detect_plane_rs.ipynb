{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RealSenseを使ってみよう（平面の自動検出）\n",
    "### RealSenseを接続してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure color and depth to run at VGA resolution at 30 frames per second\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算の準備しよう。\n",
    "\"?????\" を正しい計算式に書き換えてから実行してください。乗算は *，除算は / です。\n",
    "- X = ?????   # Z, u, f による計算式\n",
    "- Y = ?????   # Z, v, f による計算式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length = intr.fx # in pixels\n",
    "\n",
    "def Zuv_to_XY(Z, u, v, f=focal_length):\n",
    "    X = ?????   # Z, u, f による計算式\n",
    "    Y = ?????   # Z, v, f による計算式\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平面を検出するRANSACの実装\n",
    "\n",
    "def DetectPlane(points, n_trials=30, th=3):\n",
    "\n",
    "    # initial settings\n",
    "    plane = dict(normal=None, p3idx=None)\n",
    "    n_max, dev_min = 0, float(\"inf\")\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # randomly pick up three points\n",
    "        p3idx = np.random.choice(points.shape[0], 3, replace=False)\n",
    "        # compute a unit normal vector\n",
    "        normal = np.cross(points[p3idx[1]] - points[p3idx[0]], points[p3idx[2]] - points[p3idx[0]])\n",
    "        normal = normal / np.linalg.norm(normal)\n",
    "\n",
    "        # compute distances from the plane with a point p3idx[0] and the normal vector\n",
    "        distances = np.abs(np.dot(points - points[p3idx[0],:], normal))\n",
    "        \n",
    "        # find the neighboring points to the plane\n",
    "        pidx_neighbors = np.where(distances < th)[0]\n",
    "        num_neighbors = len(pidx_neighbors)\n",
    "        deviation = np.std(distances[pidx_neighbors])\n",
    "\n",
    "        # check if the plane is better than the current estimate\n",
    "        if num_neighbors > n_max or (num_neighbors == n_max and deviation < dev_min):\n",
    "            n_max, dev_min = num_neighbors, deviation\n",
    "            plane[\"normal\"], plane[\"p3idx\"] = normal, p3idx\n",
    "\n",
    "    return plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定項目（この初期設定で大丈夫です）\n",
    "disp_mm = 20.           # [mm]　この距離以内の点を平面と見なして着色する。\n",
    "max_depth = 2.0 / scale # [m]　この距離より遠い点は計算の対象にしない。\n",
    "\n",
    "n_trials = 300          # [回]　この回数だけRANSACの手順を繰り返す度に，結果を表示する。\n",
    "th = 10                 # [mm]　RANSACでは，この距離より近い点を平面にとても近いと見なして数える。\n",
    "n = 10000               # [個]　全点ではなく，ランダムに間引く。この個数の点が計算に使われる。\n",
    "mask_color = np.array((0, 0, 128),dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で準備完了です。\n",
    "\n",
    "## 平面の自動検出を実行しよう。\n",
    "- 平面として検出された領域が着色されて表示されます。\n",
    "- スペースキーを押す毎に画像が連番で保存されます。'q' を押すと終了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平面の自動検出\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# Get camera parameters\n",
    "intr = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()\n",
    "scale = config.resolve(rs.pipeline_wrapper(pipeline)).get_device().first_depth_sensor().get_depth_scale()\n",
    "cx, cy = intr.ppx, intr.ppy # width*0.5, height*0.5\n",
    "height = intr.height\n",
    "width = intr.width\n",
    "j_to_u = lambda j: -(j - cx)\n",
    "i_to_v = lambda i: -(i - cy)\n",
    "u, v = np.meshgrid(j_to_u(np.arange(width)), i_to_v(np.arange(height)))\n",
    "\n",
    "# Create a camera alignment object (depth aligned to color)\n",
    "align = rs.align(rs.stream.color)\n",
    "print(\"Press [SPACE] to save images (png) and depth data (npy).\")\n",
    "print(\"Press 'q' to stop.\")\n",
    "nsaved = 0\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        bgr = np.asanyarray(color_frame.get_data())\n",
    "        depth = np.asanyarray(depth_frame.get_data())\n",
    "        depth[depth > max_depth] = 0 # Zeros out\n",
    "        \n",
    "        Z = depth * scale * 1e+3 # unit in mm\n",
    "        X, Y = Zuv_to_XY(Z, u, v, intr.fx)\n",
    "        points = np.vstack((X[Z>0],Y[Z>0],Z[Z>0])).T\n",
    "        nd = np.count_nonzero(Z)\n",
    "        p = np.random.choice(nd, min(n,nd), replace=False)\n",
    "        points = points[p]\n",
    "        plane = DetectPlane(points, n_trials=n_trials, th=th)\n",
    "        tmp = np.vstack((X.flatten(),Y.flatten(),Z.flatten())).T\n",
    "        distances = np.abs(np.dot(tmp - points[plane['p3idx'][0],:], plane['normal']))\n",
    "        \n",
    "        distances = distances.reshape(480,640)\n",
    "        mask = np.zeros(480* 640* 3).reshape(480, 640, 3)\n",
    "        mask[distances < disp_mm] = mask_color\n",
    "        bgr = bgr.astype(np.uint32) + mask.astype(np.uint32)\n",
    "        bgr[bgr > 255] = 255\n",
    "        bgr = bgr.astype(np.uint8)\n",
    "        \n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U), \n",
    "                                           cv2.COLORMAP_JET)\n",
    "        \n",
    "        images = np.hstack((bgr, depth_colormap))\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        \n",
    "        key = cv2.waitKey(100)\n",
    "        if key == ord(' '):\n",
    "            Z = depth * scale * 1e+3 # unit in mm\n",
    "            color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Save images\n",
    "            cv2.imwrite('color{:02d}pc.png'.format(nsaved), bgr)\n",
    "            cv2.imwrite('depth{:02d}pc.png'.format(nsaved), depth_colormap)\n",
    "            np.save('Z{:02d}pc.npy'.format(nsaved), Z)\n",
    "            \n",
    "            print(\"color image and depth data are saved ({:02d})\".format(nsaved))\n",
    "            nsaved += 1\n",
    "\n",
    "        if key == ord('q'):\n",
    "            if nsaved == 0:\n",
    "                Z = depth * scale * 1e+3 # unit in mm\n",
    "                color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
